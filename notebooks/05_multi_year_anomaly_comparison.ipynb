{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Year Anomaly Comparison (2017-2019)\n",
        "\n",
        "Este notebook compara anomalias detectadas nos datasets de 2017, 2018 e 2019.\n",
        "\n",
        "**Objetivos:**\n",
        "- Carregar datasets de múltiplos anos (2017, 2018, 2019)\n",
        "- Extrair janelas de anomalias de cada dataset\n",
        "- Visualizar anomalias usando as funções refatoradas do módulo `wqdab`\n",
        "- Comparar padrões de anomalias entre anos\n",
        "- Criar visualizações comparativas\n",
        "\n",
        "**Funções utilizadas:**\n",
        "- `wqdab.data.load_single_dataset_*` - Carregar datasets\n",
        "- `wqdab.utils.data_utils` - Preparação de dados\n",
        "- `wqdab.visualization.anomaly_plots` - Visualização de anomalias\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and setup\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# WQDAB package imports\n",
        "from wqdab.data import (\n",
        "    load_single_dataset_2017,\n",
        "    load_single_dataset_2018,\n",
        "    load_single_dataset_2019,\n",
        ")\n",
        "from wqdab.utils.data_utils import (\n",
        "    prepare_time_series,\n",
        "    get_standard_sensors,\n",
        "    load_and_prepare_dataset,\n",
        ")\n",
        "from wqdab.visualization import (\n",
        "    get_anomaly_windows,\n",
        "    plot_anomaly_zoom,\n",
        "    plot_all_anomaly_windows,\n",
        ")\n",
        "from wqdab.utils.paths import ensure_directories_exist, FIGURES_DIR\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📁 FIGURES_DIR: /home/nelso/Documents/IA - Detecção Falhas/reports/figures\n"
          ]
        }
      ],
      "source": [
        "# Ensure directories exist\n",
        "ensure_directories_exist()\n",
        "print(f\"📁 FIGURES_DIR: {FIGURES_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Prepare Data\n",
        "\n",
        "Carregamos os datasets de 2017, 2018 e 2019, aplicando limpeza e preparação padrão.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "📊 YEAR 2017\n",
            "============================================================\n",
            "📥 Loading dataset for year 2017...\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 YEAR 2017\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df_train_2017, df_test_2017, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_single_dataset_2017\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2017\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Get sensors list\u001b[39;00m\n\u001b[1;32m     11\u001b[0m sensors_2017 \u001b[38;5;241m=\u001b[39m get_standard_sensors(df_train_2017)\n",
            "File \u001b[0;32m~/Documents/IA - Detecção Falhas/wqdab/utils/data_utils.py:143\u001b[0m, in \u001b[0;36mload_and_prepare_dataset\u001b[0;34m(load_func, year, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(load_func, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mload_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     data \u001b[38;5;241m=\u001b[39m load_func\n",
            "File \u001b[0;32m~/Documents/IA - Detecção Falhas/wqdab/data/data.py:4\u001b[0m, in \u001b[0;36mload_single_dataset_2017\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_single_dataset_2017\u001b[39m():\n\u001b[0;32m----> 4\u001b[0m   df_2017_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://drive.google.com/uc?export=download&id=1-EYCMAdgUODc_i37E8pPkQ_37COsEJp-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m   df_2017_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://drive.google.com/uc?export=download&id=1-Md7j1sksTUpJUKJJCWXI058gGsuHqTT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m df_2017_train, df_2017_test\n",
            "File \u001b[0;32m~/miniconda3/envs/tastefinder/lib/python3.10/site-packages/pandas/io/parquet.py:653\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    502\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    656\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m         )\n",
            "File \u001b[0;32m~/miniconda3/envs/tastefinder/lib/python3.10/site-packages/pandas/io/parquet.py:68\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     66\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
            "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
          ]
        }
      ],
      "source": [
        "# Load 2017 dataset\n",
        "print(\"=\" * 60)\n",
        "print(\"📊 YEAR 2017\")\n",
        "print(\"=\" * 60)\n",
        "df_train_2017, df_test_2017, _ = load_and_prepare_dataset(\n",
        "    load_single_dataset_2017,\n",
        "    year=2017\n",
        ")\n",
        "\n",
        "# Get sensors list\n",
        "sensors_2017 = get_standard_sensors(df_train_2017)\n",
        "print(f\"Sensors: {sensors_2017}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "📊 YEAR 2018\n",
            "============================================================\n",
            "📥 Loading dataset for year 2018...\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 YEAR 2018\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df_train_2018, df_test_2018, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_single_dataset_2018\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2018\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m sensors_2018 \u001b[38;5;241m=\u001b[39m get_standard_sensors(df_train_2018)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSensors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msensors_2018\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/IA - Detecção Falhas/wqdab/utils/data_utils.py:143\u001b[0m, in \u001b[0;36mload_and_prepare_dataset\u001b[0;34m(load_func, year, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(load_func, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mload_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     data \u001b[38;5;241m=\u001b[39m load_func\n",
            "File \u001b[0;32m~/Documents/IA - Detecção Falhas/wqdab/data/data.py:9\u001b[0m, in \u001b[0;36mload_single_dataset_2018\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_single_dataset_2018\u001b[39m():\n\u001b[0;32m----> 9\u001b[0m   df_2018_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://drive.google.com/uc?export=download&id=1-XfmTdzm16Zx4fvtu4Uz5mV8Xjeo2c2L\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m   df_2018_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://drive.google.com/uc?export=download&id=1-VX8DicXctHvhv3RZ3rCB3qHRLCsri74\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m df_2018_train, df_2018_test\n",
            "File \u001b[0;32m~/miniconda3/envs/tastefinder/lib/python3.10/site-packages/pandas/io/parquet.py:653\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    502\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    656\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m         )\n",
            "File \u001b[0;32m~/miniconda3/envs/tastefinder/lib/python3.10/site-packages/pandas/io/parquet.py:68\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     66\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
            "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
          ]
        }
      ],
      "source": [
        "# Load 2018 dataset\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📊 YEAR 2018\")\n",
        "print(\"=\" * 60)\n",
        "df_train_2018, df_test_2018, _ = load_and_prepare_dataset(\n",
        "    load_single_dataset_2018,\n",
        "    year=2018\n",
        ")\n",
        "\n",
        "sensors_2018 = get_standard_sensors(df_train_2018)\n",
        "print(f\"Sensors: {sensors_2018}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load 2019 dataset\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📊 YEAR 2019\")\n",
        "print(\"=\" * 60)\n",
        "df_train_2019, df_test_2019, df_val_2019 = load_and_prepare_dataset(\n",
        "    load_single_dataset_2019,\n",
        "    year=2019\n",
        ")\n",
        "\n",
        "sensors_2019 = get_standard_sensors(df_train_2019)\n",
        "print(f\"Sensors: {sensors_2019}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detect Anomaly Windows\n",
        "\n",
        "Extraímos janelas de anomalias de cada dataset usando a função refatorada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine train and test for each year for complete analysis\n",
        "df_2017_all = pd.concat([df_train_2017, df_test_2017]).sort_index()\n",
        "df_2018_all = pd.concat([df_train_2018, df_test_2018]).sort_index()\n",
        "df_2019_all = pd.concat([df_train_2019, df_test_2019, df_val_2019]).sort_index()\n",
        "\n",
        "print(\"📊 Dataset shapes after combining train/test/val:\")\n",
        "print(f\"  2017: {df_2017_all.shape}\")\n",
        "print(f\"  2018: {df_2018_all.shape}\")\n",
        "print(f\"  2019: {df_2019_all.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get anomaly windows for each year\n",
        "print(\"\\n🔍 Detecting anomaly windows...\\n\")\n",
        "\n",
        "windows_2017 = get_anomaly_windows(df_2017_all, margin_minutes=120)\n",
        "windows_2018 = get_anomaly_windows(df_2018_all, margin_minutes=120)\n",
        "windows_2019 = get_anomaly_windows(df_2019_all, margin_minutes=120)\n",
        "\n",
        "print(f\"✅ 2017: Found {len(windows_2017)} anomaly windows\")\n",
        "print(f\"✅ 2018: Found {len(windows_2018)} anomaly windows\")\n",
        "print(f\"✅ 2019: Found {len(windows_2019)} anomaly windows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Anomalies - First 5 Windows per Year\n",
        "\n",
        "Vamos visualizar as primeiras 5 janelas de anomalias de cada ano para comparação.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize first 5 anomaly windows from 2017\n",
        "print(\"\\n📊 Visualizing 2017 anomalies...\")\n",
        "for i in range(min(5, len(windows_2017))):\n",
        "    start, end, _, _ = windows_2017[i]\n",
        "    plot_anomaly_zoom(\n",
        "        df_2017_all, start, end,\n",
        "        sensors=get_standard_sensors(df_2017_all),\n",
        "        figsize=(14, 10),\n",
        "        save_path=FIGURES_DIR / f'anomaly_comparison_2017_window_{i}.png',\n",
        "        show=False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize first 5 anomaly windows from 2018\n",
        "print(\"\\n📊 Visualizing 2018 anomalies...\")\n",
        "for i in range(min(5, len(windows_2018))):\n",
        "    start, end, _, _ = windows_2018[i]\n",
        "    plot_anomaly_zoom(\n",
        "        df_2018_all, start, end,\n",
        "        sensors=get_standard_sensors(df_2018_all),\n",
        "        figsize=(14, 10),\n",
        "        save_path=FIGURES_DIR / f'anomaly_comparison_2018_window_{i}.png',\n",
        "        show=False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize first 5 anomaly windows from 2019\n",
        "print(\"\\n📊 Visualizing 2019 anomalies...\")\n",
        "for i in range(min(5, len(windows_2019))):\n",
        "    start, end, _, _ = windows_2019[i]\n",
        "    plot_anomaly_zoom(\n",
        "        df_2019_all, start, end,\n",
        "        sensors=get_standard_sensors(df_2019_all),\n",
        "        figsize=(14, 10),\n",
        "        save_path=FIGURES_DIR / f'anomaly_comparison_2019_window_{i}.png',\n",
        "        show=False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics\n",
        "\n",
        "Vamos comparar estatísticas de anomalias entre os diferentes anos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare anomaly statistics across years\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📊 ANOMALY STATISTICS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "years_data = {\n",
        "    '2017': (df_2017_all, windows_2017),\n",
        "    '2018': (df_2018_all, windows_2018),\n",
        "    '2019': (df_2019_all, windows_2019),\n",
        "}\n",
        "\n",
        "for year, (df, windows) in years_data.items():\n",
        "    total_samples = len(df)\n",
        "    anomaly_samples = df['EVENT'].sum()\n",
        "    anomaly_rate = (anomaly_samples / total_samples) * 100\n",
        "    n_windows = len(windows)\n",
        "    \n",
        "    avg_window_duration = np.mean([\n",
        "        (end - start).total_seconds() / 3600  # hours\n",
        "        for start, end, _, _ in windows\n",
        "    ]) if windows else 0\n",
        "    \n",
        "    print(f\"\\n{year}:\")\n",
        "    print(f\"  Total samples: {total_samples:,}\")\n",
        "    print(f\"  Anomaly samples: {anomaly_samples:,} ({anomaly_rate:.2f}%)\")\n",
        "    print(f\"  Anomaly windows: {n_windows}\")\n",
        "    print(f\"  Avg window duration: {avg_window_duration:.2f} hours\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tastefinder",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
