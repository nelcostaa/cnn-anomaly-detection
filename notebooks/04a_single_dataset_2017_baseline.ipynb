{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pYAO3OzJxJRq"
      },
      "outputs": [],
      "source": [
        "# !pip install wqdab > package.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ci301ZZdxX10"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "from imblearn.ensemble import RUSBoostClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold, cross_val_predict\n",
        "\n",
        "from wqdab.data import load_single_dataset_2017\n",
        "from wqdab.metrics import compute_metrics\n",
        "from wqdab.utils import preprocess_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_nHrJFI_xtA1"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = load_single_dataset_2017()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Uxs3Ei_Eyh3d"
      },
      "outputs": [],
      "source": [
        "X_train, X_train_orig, y_train, means, stds = preprocess_data(df_train)\n",
        "X_test, X_test_orig, y_test, _, _ = preprocess_data(df_test, means=means, stds=stds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8b5JRhrG0PUb"
      },
      "outputs": [],
      "source": [
        "def create_sliding_window(dataset, target, window_size, stride=1):\n",
        "    \"\"\"\n",
        "    Transform a tabular dataset into sliding windows for time series analysis.\n",
        "\n",
        "    Args:\n",
        "        dataset (np.ndarray or pd.DataFrame): Input features of shape (n_samples, n_features).\n",
        "        target (np.ndarray or pd.Series): Target variable of shape (n_samples,).\n",
        "        window_size (int): The size of the sliding window.\n",
        "        stride (int): The step size for sliding the window.\n",
        "\n",
        "    Returns:\n",
        "        X_windows (np.ndarray): Features reshaped with sliding windows.\n",
        "        y_windows (np.ndarray): Targets corresponding to each window.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    n_samples = len(dataset)\n",
        "\n",
        "    for start in range(0, n_samples - window_size + 1, stride):\n",
        "        end = start + window_size\n",
        "        X.append(dataset[start:end])  # Collect the window\n",
        "        y.append(target[end - 1])    # Use the target at the last time step of the window\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SFNF9Ksd02f9"
      },
      "outputs": [],
      "source": [
        "window_size = 30\n",
        "stride = 1\n",
        "\n",
        "X_train_ts, y_train_ts = create_sliding_window(X_train, y_train, window_size, stride)\n",
        "X_test_ts, y_test_ts = create_sliding_window(X_test, y_test, window_size, stride)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xp-nRjLX1oFC"
      },
      "outputs": [],
      "source": [
        "def compute_window_features(X_windows):\n",
        "    \"\"\"\n",
        "    Compute features for each sliding window.\n",
        "\n",
        "    Args:\n",
        "        X_windows (np.ndarray): Sliding window dataset of shape (n_windows, window_size, n_features).\n",
        "\n",
        "    Returns:\n",
        "        X_processed (np.ndarray): Processed dataset with shape (n_windows, 2 * n_features),\n",
        "                                  where each row contains the mean and std dev of each feature.\n",
        "    \"\"\"\n",
        "    # Compute statistics along the window dimension (axis=1)\n",
        "    mean = np.mean(X_windows, axis=1)\n",
        "    std = np.std(X_windows, axis=1)\n",
        "    xmax = np.max(X_windows, axis=1)\n",
        "    xmin = np.min(X_windows, axis=1)\n",
        "    last = X_windows[:,-1,:]\n",
        "    first = X_windows[:,0,:]\n",
        "\n",
        "    # Compute features and concatenate them to form a single feature vector per window\n",
        "    X_processed = np.concatenate([\n",
        "        mean,\n",
        "        std,\n",
        "        last - mean,\n",
        "        last - xmax,\n",
        "        last - xmin,\n",
        "        last - first,\n",
        "        last - mean\n",
        "    ], axis=1)\n",
        "    return X_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Dty7p4aD4YTT"
      },
      "outputs": [],
      "source": [
        "X_train_f = compute_window_features(X_train_ts)\n",
        "X_test_f = compute_window_features(X_test_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial, X, y):\n",
        "    # Define hyperparameter search space\n",
        "    param = {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, step=100),\n",
        "        \"sampling_strategy\": trial.suggest_float(\"sampling_strategy\", 0.1, 1.0),\n",
        "    }\n",
        "\n",
        "    # Cross-validate RUSBoost model\n",
        "    decision_tree = DecisionTreeClassifier(max_depth=trial.suggest_int(\"max_depth\", 1, 10))\n",
        "    model = RUSBoostClassifier(estimator=decision_tree, **param)\n",
        "    y_pred = cross_val_predict(model, X, y, cv=KFold())\n",
        "\n",
        "    # Use F1-score or AUC as the optimization metric\n",
        "    return f1_score(y, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9SzhBiV95JpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.03590089290530597, 'n_estimators': 100, 'sampling_strategy': 0.12060437225588891, 'max_depth': 10}\n",
            "\n",
            "    Metrics:\n",
            "    \tF1 score (classic): 0.6344\n",
            "    \tF1 score (optimistic): 0.6015\n",
            "    \tF1 score (early): 0.5714\n",
            "    \n",
            "    \tRecall score (classic): 0.7335\n",
            "    \tRecall score (optimistic): 0.8718\n",
            "    \tRecall score (early): 0.7562\n",
            "    \n",
            "    \tPrecision score (classic): 0.5589\n",
            "    \tPrecision score (range): 0.4592\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "X_train_full = np.vstack((X_train_f))\n",
        "y_train_full = np.hstack((y_train_ts))\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(lambda trial: objective(trial, X_train_full, y_train_full), n_trials=30)\n",
        "\n",
        "with open('params_2017.json', 'w') as fp:\n",
        "    json.dump(study.best_trial.params, fp)\n",
        "with open('params_2017.json', 'r') as fp:\n",
        "    best_trial_params = json.load(fp)\n",
        "print(best_trial_params)\n",
        "\n",
        "max_depth = best_trial_params['max_depth']\n",
        "params = {\n",
        "    'learning_rate': best_trial_params['learning_rate'],\n",
        "    'n_estimators': best_trial_params['n_estimators'],\n",
        "    'sampling_strategy': best_trial_params['sampling_strategy']\n",
        "}\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(max_depth=max_depth)\n",
        "model = RUSBoostClassifier(estimator=decision_tree, **params).fit(X_train_full, y_train_full)\n",
        "y_pred_test = model.predict(X_test_f)\n",
        "\n",
        "compute_metrics(y_test_ts, y_pred_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMaQfd5krrT03t/m8uHcMJd",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "research",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
